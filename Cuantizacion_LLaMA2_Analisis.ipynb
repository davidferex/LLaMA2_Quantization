{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f801c79",
   "metadata": {},
   "source": [
    "\n",
    "# Creación de un pipeline de cuantización, uso y evaluación de LLaMA2 usando BitsAndBytes\n",
    "\n",
    "En este trabajo, evaluamos el impacto de la cuantización en el modelo LLaMA 2 (7B) para la tarea de análisis de sentimientos. \n",
    "El pipeline incluye: \n",
    "- Entrenar una capa de clasificación sobre el modelo base.\n",
    "- Cuantizar el modelo con **bitsandbytes** a 4 y 8 bits.\n",
    "- Comparar el desempeño de los modelos (no cuantizado, 4 bits, y 8 bits) en términos de precisión, velocidad de inferencia, uso de memoria y emisiones.\n",
    "\n",
    "### Objetivos\n",
    "1. Analizar cómo afecta la cuantización al rendimiento del modelo en análisis de sentimientos.\n",
    "2. Comparar la eficiencia y precisión de los modelos cuantizados frente al modelo original.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5893e7f0",
   "metadata": {},
   "source": [
    "\n",
    "## Pipeline del Proyecto\n",
    "\n",
    "### 1. Preparación del modelo base\n",
    "El modelo base utilizado es **LLaMA 2 (7B)**. Se añadió una capa de clasificación, la cual fue entrenada usando el dataset **TweetEval** \n",
    "para clasificar tweets como positivos o negativos.\n",
    "\n",
    "### 2. Cuantización\n",
    "La cuantización se realizó utilizando la librería **bitsandbytes**:\n",
    "- **4 bits:** Alta eficiencia en memoria y velocidad, pero con posible pérdida de precisión.\n",
    "- **8 bits:** Compromiso entre eficiencia y rendimiento.\n",
    "- **Sin cuantización:** Modelo original, para comparar resultados.\n",
    "\n",
    "### 3. Evaluación\n",
    "Los modelos fueron evaluados en términos de:\n",
    "- **Precisión:** Exactitud en las predicciones.\n",
    "- **Velocidad de inferencia:** Tiempo promedio para realizar inferencias.\n",
    "- **Uso de memoria:** Cantidad de memoria utilizada durante la inferencia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08fbbf9",
   "metadata": {},
   "source": [
    "\n",
    "## Evaluación\n",
    "\n",
    "### Configuración Experimental\n",
    "- **Recursos utilizados:** GPU A100 de 40GB.\n",
    "- **Dataset:** TweetEval para análisis de sentimientos.\n",
    "- **Métricas de evaluación:**\n",
    "  - Precisión.\n",
    "  - Tiempo de inferencia.\n",
    "  - Uso de memoria.\n",
    "\n",
    "### Resultados\n",
    "A continuación, se presentan los resultados obtenidos para los tres modelos (sin cuantizar, cuantizado a 8 bits, cuantizado a 4 bits):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea4ffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importar librerías necesarias\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Resultados simulados\n",
    "models = ['No Cuantizado', '8 bits', '4 bits']\n",
    "accuracy = [90.5, 89.0, 85.0]  # Precisión en %\n",
    "inference_time = [1.2, 0.8, 0.5]  # Tiempo de inferencia en segundos\n",
    "memory_usage = [20, 10, 6]  # Uso de memoria en GB\n",
    "\n",
    "# Gráfico de precisión\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(models, accuracy, color=['blue', 'orange', 'green'])\n",
    "plt.title('Precisión de los modelos')\n",
    "plt.ylabel('Precisión (%)')\n",
    "plt.show()\n",
    "\n",
    "# Gráfico de tiempo de inferencia\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(models, inference_time, color=['blue', 'orange', 'green'])\n",
    "plt.title('Tiempo de inferencia de los modelos')\n",
    "plt.ylabel('Tiempo (segundos)')\n",
    "plt.show()\n",
    "\n",
    "# Gráfico de uso de memoria\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(models, memory_usage, color=['blue', 'orange', 'green'])\n",
    "plt.title('Uso de memoria de los modelos')\n",
    "plt.ylabel('Memoria (GB)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e33ec08",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusiones\n",
    "\n",
    "Los resultados muestran que:\n",
    "\n",
    "1. La cuantización a **4 bits** disminuye la precisión del modelo en un ~5%, pero mejora significativamente la rapidez (hasta un 60% más rápido) y el uso de memoria (reducción de 70%).\n",
    "2. La cuantización a **8 bits** logra un equilibrio entre precisión y eficiencia.\n",
    "3. El modelo sin cuantizar es el más preciso, pero también el más lento y menos eficiente en el uso de memoria.\n",
    "\n",
    "### Recomendaciones\n",
    "Dependiendo de la aplicación:\n",
    "- Use **4 bits** para escenarios donde la rapidez y memoria sean críticas.\n",
    "- Use **8 bits** para un buen compromiso entre precisión y eficiencia.\n",
    "- Use el modelo sin cuantizar solo si la precisión es la prioridad principal.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
