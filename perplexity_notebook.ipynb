{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ef11ff9",
   "metadata": {},
   "source": [
    "# Notebook to replicate perplexity results\n",
    "\n",
    "In this notebook we will show the code and all the explanations and considerations needed to replicate the results obtained for perplexity with LLaMA 2 7b and Wikitext2 dataset in a text generation task.\n",
    "\n",
    "Firstly, we need to set up a python environment. For this purpose, the usual command can be used. For example:\n",
    "\n",
    "python3 -m venv .env\n",
    "\n",
    "After we have the environment created, we need to install all the requirements. For this, just execute the following command with the environment activated:\n",
    "\n",
    "pip install -r requirements.txt\n",
    "\n",
    "After we have all set up, we just need to import everything and compute perplexity for each quantization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd88f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GPTQConfig, HqqConfig\n",
    "from optimum.quanto import QuantizedModelForCausalLM, qint4, qint8\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from codecarbon import OfflineEmissionsTracker\n",
    "import math\n",
    "import os\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec07819",
   "metadata": {},
   "source": [
    "Now let's first calculate perplexity for the model without quantization. We define the model loading function for BitsAndBytes but that allows loading without quantization.\n",
    "\n",
    "Clearly, the lines related to the use of CodeCarbon and Wandb can be changed or even commented out. They can be customized as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf63cb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'  # Change depending on GPU used\n",
    "\n",
    "tracker1 = OfflineEmissionsTracker(country_iso_code=\"ESP\", allow_multiple_runs = True, output_file= \"./emissions_not_quant_perplexity.csv\", gpu_ids=[1])\n",
    "wandb.init(project=\"Perplexity\", name=\"not_quant\")\n",
    "\n",
    "def load_llama2_model(model_id, quantization=None):\n",
    "    if quantization == \"4bit\":\n",
    "        quant_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "        )\n",
    "    elif quantization == \"8bit\":\n",
    "        quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    else:\n",
    "        quant_config = None\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=quant_config,\n",
    "        torch_dtype=torch.float16 if quantization else torch.float32\n",
    "    )\n",
    "    return tokenizer, model\n",
    "\n",
    "def calculate_perplexity_precise(model, tokenizer, texts, max_length=1024, stride=512, device='cuda'):\n",
    "    model.eval()\n",
    "\n",
    "    nll_sum = 0.0\n",
    "    n_tokens = 0\n",
    "\n",
    "    for text in tqdm(texts, desc=\"Calculating perplexity\"):\n",
    "        encodings = tokenizer(text, return_tensors=\"pt\", truncation=False)\n",
    "        input_ids = encodings.input_ids.to(device)\n",
    "        seq_len = input_ids.size(1)\n",
    "        prev_end_loc = 0\n",
    "\n",
    "        for begin_loc in range(0, seq_len, stride):\n",
    "            end_loc = min(begin_loc + max_length, seq_len)\n",
    "            trg_len = end_loc - prev_end_loc\n",
    "\n",
    "            input_ids_chunk = input_ids[:, begin_loc:end_loc]\n",
    "            target_ids = input_ids_chunk.clone()\n",
    "            target_ids[:, :-trg_len] = -100  \n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids_chunk, labels=target_ids)\n",
    "                neg_log_likelihood = outputs.loss\n",
    "\n",
    "            valid_tokens = (target_ids != -100).sum().item()\n",
    "            effective_tokens = valid_tokens - target_ids.size(0)  \n",
    "            nll_sum += neg_log_likelihood.item() * effective_tokens\n",
    "            n_tokens += effective_tokens\n",
    "\n",
    "            prev_end_loc = end_loc\n",
    "            if end_loc == seq_len:\n",
    "                break\n",
    "\n",
    "    avg_nll = nll_sum / n_tokens\n",
    "    ppl = math.exp(avg_nll)\n",
    "    wandb.log({\"Perplexity\": ppl})\n",
    "    return ppl\n",
    "\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\" \n",
    "quantization = None  # Change to \"8bit\" or \"4bit\" if needed\n",
    "\n",
    "print(f\"Loading model {model_id} with quantization: {quantization or 'not quantized'}\")\n",
    "tracker1.start()\n",
    "tokenizer, model = load_llama2_model(model_id, quantization)\n",
    "tracker1.stop()\n",
    "\n",
    "print(\"Loading WikiText-2...\")\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test[:100%]\")\n",
    "texts = [sample[\"text\"] for sample in dataset if sample[\"text\"].strip()]\n",
    "\n",
    "print(\"Calculating perplexity...\")\n",
    "tracker2.start()\n",
    "ppl = calculate_perplexity_precise(model, tokenizer, texts)\n",
    "tracker2.stop()\n",
    "print(f\"\\n✅ Perplexity ({quantization or 'not quantized'}): {ppl:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b7292a",
   "metadata": {},
   "source": [
    "Now we proceed with BitsAndBytes (8 and 4 bits) results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87ee663",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker1 = OfflineEmissionsTracker(country_iso_code=\"ESP\", allow_multiple_runs = True, output_file= \"./emissions_8bit_ByB_quant_perplexity.csv\", gpu_ids=[1])\n",
    "tracker2 = OfflineEmissionsTracker(country_iso_code=\"ESP\", allow_multiple_runs = True, output_file= \"./emissions_8bit_ByB_quant_eval_perplexity.csv\", gpu_ids=[1])\n",
    "wandb.init(project=\"Perplexity\", name=\"8bit_ByB\")\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\" \n",
    "quantization = \"8bit\" \n",
    "\n",
    "print(f\"Loading model {model_id} with quantization: {quantization or 'not quantized'}\")\n",
    "tracker1.start()\n",
    "tokenizer, model = load_llama2_model(model_id, quantization)\n",
    "tracker1.stop()\n",
    "\n",
    "print(\"Loading WikiText-2...\")\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test[:100%]\")\n",
    "texts = [sample[\"text\"] for sample in dataset if sample[\"text\"].strip()]\n",
    "\n",
    "print(\"Calculating perplexity...\")\n",
    "tracker2.start()\n",
    "ppl = calculate_perplexity_precise(model, tokenizer, texts)\n",
    "tracker2.stop()\n",
    "print(f\"\\n✅ Perplexity ({quantization or 'not quantized'}): {ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff5e1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker1 = OfflineEmissionsTracker(country_iso_code=\"ESP\", allow_multiple_runs = True, output_file= \"./emissions_4bit_ByB_quant_perplexity.csv\", gpu_ids=[1])\n",
    "tracker2 = OfflineEmissionsTracker(country_iso_code=\"ESP\", allow_multiple_runs = True, output_file= \"./emissions_4bit_ByB_quant_eval_perplexity.csv\", gpu_ids=[1])\n",
    "wandb.init(project=\"Perplexity\", name=\"4bit_ByB\")\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\" \n",
    "quantization = \"4bit\" \n",
    "\n",
    "print(f\"Loading model {model_id} with quantization: {quantization or 'not quantized'}\")\n",
    "tracker1.start()\n",
    "tokenizer, model = load_llama2_model(model_id, quantization)\n",
    "tracker1.stop()\n",
    "\n",
    "print(\"Loading WikiText-2...\")\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test[:100%]\")\n",
    "texts = [sample[\"text\"] for sample in dataset if sample[\"text\"].strip()]\n",
    "\n",
    "print(\"Calculating perplexity...\")\n",
    "tracker2.start()\n",
    "ppl = calculate_perplexity_precise(model, tokenizer, texts)\n",
    "tracker2.stop()\n",
    "print(f\"\\n✅ Perplexity ({quantization or 'not quantized'}): {ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b714e6",
   "metadata": {},
   "source": [
    "Now we proceed with GPTQ (8,4 and 3 bits):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d674e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llama2_model(model_id, quantization=None):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False)\n",
    "    if quantization == \"4bit\":\n",
    "        quant_config = GPTQConfig(bits=4, dataset=\"c4\", tokenizer=tokenizer)\n",
    "    elif quantization == \"8bit\":\n",
    "        quant_config = GPTQConfig(bits=8, dataset=\"c4\", tokenizer=tokenizer)\n",
    "    elif quantization == \"3bit\":\n",
    "        quant_config = GPTQConfig(bits=3, dataset=\"c4\", tokenizer=tokenizer)\n",
    "    else:\n",
    "        quant_config = None\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=quant_config,\n",
    "        torch_dtype=torch.float16 if quantization else torch.float32\n",
    "    )\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e994bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker1 = OfflineEmissionsTracker(country_iso_code=\"ESP\", allow_multiple_runs = True, output_file= \"./emissions_8bit_GPTQ_quant_perplexity.csv\", gpu_ids=[1])\n",
    "tracker2 = OfflineEmissionsTracker(country_iso_code=\"ESP\", allow_multiple_runs = True, output_file= \"./emissions_8bit_GPTQ_quant_eval_perplexity.csv\", gpu_ids=[1])\n",
    "wandb.init(project=\"Perplexity\", name=\"8bit_GPTQ\")\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\" \n",
    "quantization = \"8bit\" \n",
    "\n",
    "print(f\"Loading model {model_id} with quantization: {quantization or 'not quantized'}\")\n",
    "tracker1.start()\n",
    "tokenizer, model = load_llama2_model(model_id, quantization)\n",
    "tracker1.stop()\n",
    "\n",
    "print(\"Loading WikiText-2...\")\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test[:100%]\")\n",
    "texts = [sample[\"text\"] for sample in dataset if sample[\"text\"].strip()]\n",
    "\n",
    "print(\"Calculating perplexity...\")\n",
    "tracker2.start()\n",
    "ppl = calculate_perplexity_precise(model, tokenizer, texts)\n",
    "tracker2.stop()\n",
    "print(f\"\\n✅ Perplexity ({quantization or 'not quantized'}): {ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e932b96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker1 = OfflineEmissionsTracker(country_iso_code=\"ESP\", allow_multiple_runs = True, output_file= \"./emissions_4bit_GPTQ_quant_perplexity.csv\", gpu_ids=[1])\n",
    "tracker2 = OfflineEmissionsTracker(country_iso_code=\"ESP\", allow_multiple_runs = True, output_file= \"./emissions_4bit_GPTQ_quant_eval_perplexity.csv\", gpu_ids=[1])\n",
    "wandb.init(project=\"Perplexity\", name=\"4bit_GPTQ\")\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\" \n",
    "quantization = \"4bit\" \n",
    "\n",
    "print(f\"Loading model {model_id} with quantization: {quantization or 'not quantized'}\")\n",
    "tracker1.start()\n",
    "tokenizer, model = load_llama2_model(model_id, quantization)\n",
    "tracker1.stop()\n",
    "\n",
    "print(\"Loading WikiText-2...\")\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test[:100%]\")\n",
    "texts = [sample[\"text\"] for sample in dataset if sample[\"text\"].strip()]\n",
    "\n",
    "print(\"Calculating perplexity...\")\n",
    "tracker2.start()\n",
    "ppl = calculate_perplexity_precise(model, tokenizer, texts)\n",
    "tracker2.stop()\n",
    "print(f\"\\n✅ Perplexity ({quantization or 'not quantized'}): {ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f960067",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker1 = OfflineEmissionsTracker(country_iso_code=\"ESP\", allow_multiple_runs = True, output_file= \"./emissions_3bit_GPTQ_quant_perplexity.csv\", gpu_ids=[1])\n",
    "tracker2 = OfflineEmissionsTracker(country_iso_code=\"ESP\", allow_multiple_runs = True, output_file= \"./emissions_3bit_GPTQ_quant_eval_perplexity.csv\", gpu_ids=[1])\n",
    "wandb.init(project=\"Perplexity\", name=\"3bit_GPTQ\")\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\" \n",
    "quantization = \"3bit\" \n",
    "\n",
    "print(f\"Loading model {model_id} with quantization: {quantization or 'not quantized'}\")\n",
    "tracker1.start()\n",
    "tokenizer, model = load_llama2_model(model_id, quantization)\n",
    "tracker1.stop()\n",
    "\n",
    "print(\"Loading WikiText-2...\")\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test[:100%]\")\n",
    "texts = [sample[\"text\"] for sample in dataset if sample[\"text\"].strip()]\n",
    "\n",
    "print(\"Calculating perplexity...\")\n",
    "tracker2.start()\n",
    "ppl = calculate_perplexity_precise(model, tokenizer, texts)\n",
    "tracker2.stop()\n",
    "print(f\"\\n✅ Perplexity ({quantization or 'not quantized'}): {ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee27b33",
   "metadata": {},
   "source": [
    "Now we proceed with HQQ (8,4 and 3 bits):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec462d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_llama2_model(model_id, quantization=None):\n",
    "    if quantization == \"4bit\":\n",
    "        quant_config = HqqConfig(nbits=4, group_size=64)\n",
    "    elif quantization == \"8bit\":\n",
    "        quant_config = HqqConfig(nbits=8, group_size=64)\n",
    "    elif quantization == \"3bit\":\n",
    "        quant_config = HqqConfig(nbits=3, group_size=64)\n",
    "    else:\n",
    "        quant_config = None\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=quant_config,\n",
    "        torch_dtype=torch.float16 if quantization else torch.float32\n",
    "    )\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7f3f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker1 = OfflineEmissionsTracker(country_iso_code=\"ESP\", allow_multiple_runs = True, output_file= \"./emissions_8bit_HQQ_quant_perplexity.csv\", gpu_ids=[1])\n",
    "tracker2 = OfflineEmissionsTracker(country_iso_code=\"ESP\", allow_multiple_runs = True, output_file= \"./emissions_8bit_HQQ_quant_eval_perplexity.csv\", gpu_ids=[1])\n",
    "wandb.init(project=\"Perplexity\", name=\"8bit_HQQ\")\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\" \n",
    "quantization = \"8bit\" \n",
    "\n",
    "print(f\"Loading model {model_id} with quantization: {quantization or 'not quantized'}\")\n",
    "tracker1.start()\n",
    "tokenizer, model = load_llama2_model(model_id, quantization)\n",
    "tracker1.stop()\n",
    "\n",
    "print(\"Loading WikiText-2...\")\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test[:100%]\")\n",
    "texts = [sample[\"text\"] for sample in dataset if sample[\"text\"].strip()]\n",
    "\n",
    "print(\"Calculating perplexity...\")\n",
    "tracker2.start()\n",
    "ppl = calculate_perplexity_precise(model, tokenizer, texts)\n",
    "tracker2.stop()\n",
    "print(f\"\\n✅ Perplexity ({quantization or 'not quantized'}): {ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dba72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker1 = OfflineEmissionsTracker(country_iso_code=\"ESP\", allow_multiple_runs = True, output_file= \"./emissions_4bit_HQQ_quant_perplexity.csv\", gpu_ids=[1])\n",
    "tracker2 = OfflineEmissionsTracker(country_iso_code=\"ESP\", allow_multiple_runs = True, output_file= \"./emissions_4bit_HQQ_quant_eval_perplexity.csv\", gpu_ids=[1])\n",
    "wandb.init(project=\"Perplexity\", name=\"4bit_HQQ\")\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\" \n",
    "quantization = \"8bit\" \n",
    "\n",
    "print(f\"Loading model {model_id} with quantization: {quantization or 'not quantized'}\")\n",
    "tracker1.start()\n",
    "tokenizer, model = load_llama2_model(model_id, quantization)\n",
    "tracker1.stop()\n",
    "\n",
    "print(\"Loading WikiText-2...\")\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test[:100%]\")\n",
    "texts = [sample[\"text\"] for sample in dataset if sample[\"text\"].strip()]\n",
    "\n",
    "print(\"Calculating perplexity...\")\n",
    "tracker2.start()\n",
    "ppl = calculate_perplexity_precise(model, tokenizer, texts)\n",
    "tracker2.stop()\n",
    "print(f\"\\n✅ Perplexity ({quantization or 'not quantized'}): {ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ba1904",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker1 = OfflineEmissionsTracker(country_iso_code=\"ESP\", allow_multiple_runs = True, output_file= \"./emissions_3bit_HQQ_quant_perplexity.csv\", gpu_ids=[1])\n",
    "tracker2 = OfflineEmissionsTracker(country_iso_code=\"ESP\", allow_multiple_runs = True, output_file= \"./emissions_3bit_HQQ_quant_eval_perplexity.csv\", gpu_ids=[1])\n",
    "wandb.init(project=\"Perplexity\", name=\"3bit_HQQ\")\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\" \n",
    "quantization = \"8bit\" \n",
    "\n",
    "print(f\"Loading model {model_id} with quantization: {quantization or 'not quantized'}\")\n",
    "tracker1.start()\n",
    "tokenizer, model = load_llama2_model(model_id, quantization)\n",
    "tracker1.stop()\n",
    "\n",
    "print(\"Loading WikiText-2...\")\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test[:100%]\")\n",
    "texts = [sample[\"text\"] for sample in dataset if sample[\"text\"].strip()]\n",
    "\n",
    "print(\"Calculating perplexity...\")\n",
    "tracker2.start()\n",
    "ppl = calculate_perplexity_precise(model, tokenizer, texts)\n",
    "tracker2.stop()\n",
    "print(f\"\\n✅ Perplexity ({quantization or 'not quantized'}): {ppl:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
