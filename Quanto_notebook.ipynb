{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ebc6127",
   "metadata": {},
   "source": [
    "# Replicate Quanto results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6237992",
   "metadata": {},
   "source": [
    "In this notebook we will show the code and all the explanations and considerations needed to replicate the results obtained for Quanto with LLaMA 2 7b in sentiment analysis task.\n",
    "\n",
    "Firstly, we need to set up a python environment. For this purpose, the usual command can be used. For example:\n",
    "\n",
    "python3 -m venv .env\n",
    "\n",
    "After we have the environment created, we need to install all the requirements. For this, just execute the following command with the environment activated. This will install all the requirements for the whole project, not just this notebook.\n",
    "\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99bc822",
   "metadata": {},
   "source": [
    "Now we can start with the code. Firstly, we import all the necessary libraries and methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20511db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, LlamaForSequenceClassification, LlamaTokenizer, LlamaConfig, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from codecarbon import OfflineEmissionsTracker\n",
    "import time\n",
    "import wandb\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from optimum.quanto import QuantizedModelForCausalLM, qint4, qint8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b563a79a",
   "metadata": {},
   "source": [
    "Firsly we add a classification layer to our base model and train it using LoRA. After this, we save the model. We do this step by step. \n",
    "\n",
    "Note that through this code, some emissions trackers from CodeCarbon or Wandb functions will appear. This can be changed if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1cfde7",
   "metadata": {},
   "source": [
    "Initial configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01070975",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = \"######\" #Replace with your HugginFace token\n",
    "login(token=hf_token)\n",
    "\n",
    "run = wandb.init(project=\"LLaMA_LoRA_training\")\n",
    "\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0' #Change depending on the GPU used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776b7ed1",
   "metadata": {},
   "source": [
    "Load the dataset and the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987265bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset\n",
    "dataset = load_dataset(\"tweet_eval\", \"sentiment\")\n",
    "\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name, token=hf_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8fdb6f",
   "metadata": {},
   "source": [
    "Calculate the maximum size of the tweets in tokens and define a function to generate tokens using this maximum length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1533b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_train = max([len(tokenizer.tokenize(tweet)) for tweet in dataset['train']['text']])\n",
    "max_length_validation = max([len(tokenizer.tokenize(tweet)) for tweet in dataset['validation']['text']])\n",
    "max_length = max(max_length_train,max_length_validation)\n",
    "\n",
    "\n",
    "# Function to generate tokens\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35f83b8",
   "metadata": {},
   "source": [
    "Split the dataset for training and validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d998c359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets for training and validation\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "train_dataset = tokenized_datasets[\"train\"].remove_columns([\"text\"])\n",
    "validation_dataset = tokenized_datasets[\"validation\"].remove_columns([\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2277584d",
   "metadata": {},
   "source": [
    "Load the model for sequence classification (this adds the classification layer), create the LoRA configuration and apply LoRA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916380c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LlamaForSequenceClassification.from_pretrained(model_name, num_labels=3, token=hf_token)\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,  # Tarea objetivo: Sequence Classification\n",
    "    r=8,                         # Dimensi√≥n low-rank (ajustable)\n",
    "    lora_alpha=16,               # Factor de escalado\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]  # Aplicar LoRA a las capas attention\n",
    ")\n",
    "\n",
    "tracker1 = OfflineEmissionsTracker(country_iso_code=\"ESP\", allow_multiple_runs = True, output_file= \"./emissions_LoRA.csv\")\n",
    "tracker1.start()\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b0c203",
   "metadata": {},
   "source": [
    "Send the model to GPU and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bad8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = False\n",
    "num_tries = 0\n",
    "while not loaded:\n",
    "    try:\n",
    "        model.to(\"cuda\")\n",
    "        loaded = True\n",
    "    except torch.OutOfMemoryError:\n",
    "        num_tries += 1\n",
    "        print('New try:', num_tries)\n",
    "        time.sleep(5)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_lora\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    report_to=\"wandb\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919a8818",
   "metadata": {},
   "source": [
    "Save the model (note that this actually just saves the LoRA adapters):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464994ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Change the directory if needed\n",
    "trainer.save_model(\"./results_lora\")\n",
    "\n",
    "\n",
    "model.save_pretrained(\"./results_lora\")\n",
    "\n",
    "\n",
    "tokenizer.save_pretrained(\"./results_lora\")\n",
    "\n",
    "tracker1.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c10357a",
   "metadata": {},
   "source": [
    "Now we can quantize this model and test how it performs. Firstly, we do the initial configuration. Again, this CodeCarbon trackers and Wandb projects can be changed accordingly to your necessities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e066b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker1 = OfflineEmissionsTracker(country_iso_code=\"ESP\", allow_multiple_runs = True, output_file= \"./emissions_8bit_quant_batch64_Quanto_LORA.csv\", gpu_ids=[0])\n",
    "tracker2 = OfflineEmissionsTracker(country_iso_code=\"ESP\", allow_multiple_runs = True, output_file= \"./emissions_8bit_eval_batch64_Quanto_LORA.csv\", gpu_ids=[0])\n",
    "\n",
    "wandb.init(project=\"Quanto_Lora\", name=\"8bit_batch64\")\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "for i in range(num_gpus):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dc4369",
   "metadata": {},
   "source": [
    "Load the dataset and the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6784bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"tweet_eval\", \"sentiment\")\n",
    "\n",
    "model_name = './results_lora'   #Change the path accordingly\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    token=hf_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c698351d",
   "metadata": {},
   "source": [
    "Quantize the model. We show here the two options, 8bit and 4bits. Choose whichever you prefer and change trackers or wandb projects and runs names if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e821e2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 bit\n",
    "tracker1.start()\n",
    "# Cargar el modelo, indicando con LlamaConfig la clasificaci√≥n (por LoRA)\n",
    "config = LlamaConfig.from_pretrained(model_name, num_labels=3)\n",
    "model = LlamaForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    token=hf_token,\n",
    "    config=config,\n",
    "    device_map=\"auto\")\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model = QuantizedModelForCausalLM.quantize(model, weights=qint8, exclude='lm_head')\n",
    "tracker1.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a360af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 bit\n",
    "tracker1.start()\n",
    "# Cargar el modelo, indicando con LlamaConfig la clasificaci√≥n (por LoRA)\n",
    "config = LlamaConfig.from_pretrained(model_name, num_labels=3)\n",
    "model = LlamaForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    token=hf_token,\n",
    "    config=config,\n",
    "    device_map=\"auto\")\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model = QuantizedModelForCausalLM.quantize(model, weights=qint4, exclude='lm_head')\n",
    "tracker1.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d6c145",
   "metadata": {},
   "source": [
    "Send model to GPU and main funcion to evaluate the model in terms of accuracy and inference time:Main function to evaluate the model in terms of accuracy and time of inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d32be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loaded = False\n",
    "num_tries = 0\n",
    "while not loaded:\n",
    "    try:\n",
    "        model.to(\"cuda\")\n",
    "        loaded = True\n",
    "    except torch.OutOfMemoryError:\n",
    "        num_tries += 1\n",
    "        print('New try:', num_tries)\n",
    "        time.sleep(5)\n",
    "\n",
    "        \n",
    "def evaluate_model(model, tokenizer, dataset, batch_size=8):\n",
    "    model.eval()\n",
    "\n",
    "    start_time = time.time()\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    max_memory_usage = 0\n",
    "\n",
    "    dataset.set_format(\"torch\")\n",
    "    test_dataset = dataset[\"test\"]\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    max_length = max([len(tokenizer.tokenize(tweet)) for tweet in test_dataset[\"text\"]])\n",
    "\n",
    "    for batch in tqdm(test_loader, desc=\"Test\"):\n",
    "        torch.cuda.synchronize()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "        inputs = tokenizer(batch['text'], return_tensors=\"pt\",\n",
    "                           padding=\"max_length\", truncation=True, max_length=max_length).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            pred_batch = torch.argmax(outputs.logits, dim=-1).cpu().tolist()\n",
    "\n",
    "        memory_used = torch.cuda.max_memory_allocated() / (1024 ** 2) \n",
    "        max_memory_usage = max(max_memory_usage, memory_used)\n",
    "\n",
    "        true_labels.extend(batch['label'])\n",
    "        pred_labels.extend(pred_batch)\n",
    "\n",
    "    end_time = time.time()\n",
    "    inference_time = end_time - start_time\n",
    "\n",
    "    print(classification_report(true_labels, pred_labels, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))\n",
    "\n",
    "    wandb.log({\"Max_memory_usage_MB\": max_memory_usage})\n",
    "\n",
    "    return inference_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906b4db1",
   "metadata": {},
   "source": [
    "Evaluate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa3b3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker2.start()\n",
    "time_no = evaluate_model(model, tokenizer, dataset, 64)\n",
    "tracker2.stop()\n",
    "\n",
    "print(f\"Quantized LoRA model quanto 8bit - Inference time: {time_no:.2f}s\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
